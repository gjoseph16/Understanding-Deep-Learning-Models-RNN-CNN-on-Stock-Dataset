# -*- coding: utf-8 -*-
"""DL on Stock Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bE04cFUtePAR6soSouEDlyCm0F12Bqbg
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import scale
from os.path import join
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, mean_absolute_error as mae
import os
!pip3 install pathlib2
from pathlib2 import Path
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import display, HTML
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.model_selection import cross_val_score
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from sklearn.ensemble import StackingClassifier
from tqdm.notebook import tqdm
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import roc_curve
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_auc_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers, models, backend as K, callbacks

"""#ALL FUNCTIONS USED IN CODE """

def load_dataset(file_path):
    """
    Load a single dataset.
    """
    df_data = pd.read_csv(file_path, index_col="Date")
    return df_data

def load_aggregated_datasets(file_paths):
    """
    Load and pre-process datasets from various markets.
    """
    
    market_orders = [] # store the order which markets are processed
    n_markets = 0 # number of markets used
    aggregated_datasets = {}
    
    # Iterate over different indices data to load and process them
    for file_path in file_paths:
        df_data = load_dataset(file_path)
        
        ## Store information on the order of datasets that are processed
        data_name = df_data["Name"][0]
        print(df_data["Name"][0])
        market_orders.append(data_name)
        del df_data["Name"]
        n_markets += 1
        
        ## Preprocess data
        label = (df_data["Close"][1:] / df_data["Close"][:-1].values).astype(int)
        df_data = df_data[:-1]
        label.index = df_data.index
        
        # do not use the first 200 data as we use moving average as one of the feature
        df_data = df_data[200:]
        df_data[CLASS_NAME] = label
        
        ## Store in dictionary
        aggregated_datasets[data_name] = df_data
    
    return market_orders, n_markets, aggregated_datasets




# The above code generate vanilla datasets, the following generate
# sequential datasets.
def generate_sequential_data(df_data, sequence_length):
    """
    Given a dataframe and sequence length, generate sequential data.
    """
    
    label = list(df_data[CLASS_NAME])
    df_data = df_data.drop(columns=[CLASS_NAME])
    sequential_data = [] # used to store sequential data
    sequential_target = []
    
    ## Sequencing data
    for idx in range(df_data.shape[0]-sequence_length+1):
        sequential_data.append(df_data[idx:idx+sequence_length])
        sequential_target.append(label[idx+sequence_length-1])
    
    ## Notes:
    # - If using conv net, add 1 dimension by reshape later.
    # - convert the sequential_data list of df to np array later
    
    return sequential_data, sequential_target


# The above code generate vanilla datasets, the following generate
# sequential datasets.
def generate_sequential_data_3d(data, target, sequence_length):
    """
    Given a dataframe and sequence length, generate sequential data (for 3d cnn pred).
    """
    sequential_data = []
    sequential_target = []
    
    ## Sequencing data
    for idx in range(data.shape[1]-sequence_length+1):
        sequential_data.append(data[:, idx:idx+sequence_length])
        sequential_target.append(target[idx+sequence_length-1])
    
    sequential_data = np.array(sequential_data)
    sequential_target = np.array(sequential_target)
    
    return sequential_data, sequential_target

def generate_all_sequential_data(sequence_length, df_datas=None):
    """
    Generate aggregated dataset from all markets.
    """
    
    sequential_data = []
    sequential_target = []
    
    # Load datasets
    ## If the datasets are not specified, used all datas
    if df_datas is None:
        market_orders, n_markets, aggregated_datasets = load_aggregated_datasets([DATASET_DJI, 
                                                                              DATASET_NASDAQ, 
                                                                              DATASET_NYSE,
                                                                              DATASET_RUSSELL, 
                                                                              DATASET_SP])
        
        # Iterate over all datasets and generate sequential version of it
        for market in market_orders:
            seq_data, seq_target = generate_sequential_data(aggregated_datasets[market], sequence_length)
            sequential_data.extend(seq_data)
            sequential_target.extend(seq_target)
            
    else:
        df_datas = df_datas
        
        for df_data in df_datas:
            seq_data, seq_target = generate_sequential_data(df_data, sequence_length)
            sequential_data.extend(seq_data)
            sequential_target.extend(seq_target)

    
    return sequential_data, sequential_target

def sequential_reshape(X_seq, reshape_size):
    """
    Reshape sequential data into required format.
    """
    
    X_seq_new = [X_seq[i].to_numpy() for i in range(len(X_seq))]
    X_seq_new = np.array(X_seq_new)
    X_seq_new = X_seq_new.reshape(reshape_size)
    
    return X_seq_new

def analyse_cv(model, X_train, y_train, cv, scoring):
    """
    Do cross validation and compute relevant statistics.
    """
    
    scores = cross_val_score(model, X_train, y_train,
                             scoring=scoring, cv=cv)
    
    print("Scores:", scores)
    print("Mean Scores:", scores.mean())
    print("Standard deviation:", scores.std())

def stacking_classifier():
    """
    Create stack classifier.
    """
    
    # define the base models
    level0 = list()
    level0.append(('lr', LogisticRegression()))
    level0.append(('knn', KNeighborsClassifier()))
    level0.append(('cart', DecisionTreeClassifier()))
    level0.append(('svm', SVC()))
    level0.append(('bayes', GaussianNB()))

    # define meta learner model
    level1 = LogisticRegression()

    # define the stacking ensemble
    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)
    
    return model

def cnnpred_2d(sequence_length, n_feature, n_filters, dropout_rate=0.1):
    """
    Build model using architecture that is specified on the paper
    (Hoseinzade and Haratizadeh).
    """
    
    model = keras.Sequential([
        # Layer 1
        keras.Input(shape=(sequence_length, n_feature, 1)),
        layers.Conv2D(n_filters[0], (1, n_feature), activation="relu"),
        
        # Layer 2
        layers.Conv2D(n_filters[1], (3, 1), activation="relu"),
        layers.MaxPool2D(pool_size=(2, 1)),
        
        # Layer 3
        layers.Conv2D(n_filters[2], (3, 1), activation="relu"),
        layers.MaxPool2D(pool_size=(2, 1)),
        
        # FFNN
        layers.Flatten(),
        layers.Dropout(dropout_rate),
        layers.Dense(1, activation="sigmoid")
    ])
    
    return model


def cnnpred_3d(n_markets, sequence_length, n_feature, n_filters):
    """
    Build model using architecture that is specified on the paper
    (Hoseinzade and Haratizadeh).
    """
    
    model = keras.Sequential([
        # layer 1
        layers.Conv2D(n_filters[0], (1, 1), activation='relu', 
                      input_shape=(n_markets,sequence_length,n_feature), data_format='channels_last'),
        
        # layer 2
        layers.Conv2D(n_filters[1], (n_markets, 3), activation="relu"),
        layers.MaxPool2D(pool_size=(1, 2)),
        
        # layer 3
        layers.Conv2D(n_filters[2], (1, 3), activation="relu"),
        layers.MaxPool2D(pool_size=(1, 2)),
        
        # FFNN
        layers.Flatten(),
        layers.Dropout(0.1),
        layers.Dense(1, activation="sigmoid")
    ])
    
    return model

def lstm(win_length, num_features):
    """
    Build LSTM model for predicting stock market direction.
    """
    
    model = tf.keras.Sequential()
    model.add(layers.LSTM(128, input_shape=(win_length, num_features), return_sequences=True))
    model.add(layers.LeakyReLU(alpha=0.5))
    model.add(layers.Dropout(0.3))
    model.add(layers.LSTM(64, return_sequences=False))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(1, activation="sigmoid"))
    
    return model

def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        """Recall metric.

        Only computes a batch-wise average of recall.

        Computes the recall, a metric for multi-label classification of
        how many relevant items are selected.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        """Precision metric.

        Only computes a batch-wise average of precision.

        Computes the precision, a metric for multi-label classification of
        how many selected items are relevant.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

    precision_pos = precision(y_true, y_pred)
    recall_pos = recall(y_true, y_pred)
    precision_neg = precision((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))
    recall_neg = recall((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))
    f_posit = 2 * ((precision_pos * recall_pos) / (precision_pos + recall_pos + K.epsilon()))
    f_neg = 2 * ((precision_neg * recall_neg) / (precision_neg + recall_neg + K.epsilon()))

    return (f_posit + f_neg) / 2

def plot_confusion_matrix(y_test, y_predict, labels, cmap="Blues"):
    # Plot confusion matrix and normalised confusion matrix
    fig1 = plt.figure(figsize=(4,4))
    fig2 = plt.figure(figsize=(4,4))
    ax1 = fig1.add_subplot(111)
    ax2 = fig2.add_subplot(111)
    cm = confusion_matrix(y_test, y_predict)
    disp = ConfusionMatrixDisplay(cm)
    disp = disp.plot(ax=ax1, cmap=cmap)
    cm_n = confusion_matrix(y_test, y_predict, normalize="true")
    disp_n = ConfusionMatrixDisplay(cm_n)
    disp_n = disp_n.plot(ax=ax2, cmap=cmap)
            
    ax1.set_xticklabels(labels)
    ax1.set_yticklabels(labels, rotation=90)
    ax2.set_xticklabels(labels)
    ax2.set_yticklabels(labels, rotation=90)
    ax1.set_xlabel("$Predicted$")
    ax2.set_xlabel("$Predicted$")
    ax1.set_ylabel("$True$")
    ax2.set_ylabel("$True$")
    plt.show()


def plot_pr_vs_threshold(precisions, recalls, thresholds):
    """
    Plot precision and recall against threshold.
    """
    
    fig = plt.figure(figsize=(6,4))
    ax = plt.subplot2grid((1,1), (0,0))
    ax.plot(thresholds, precisions[:-1], "b--", label="Precision")
    ax.plot(thresholds, recalls[:-1], "g--", label="Recall")
    plt.xlabel("Threshold")
    plt.legend(loc="upper right")
    plt.ylim([0, 1])
    
    return ax

def plot_roc_curve(fpr, tpr, label=None):
    """
    Plot roc curve.
    """
    
    fig = plt.figure(figsize=(6,4))
    ax = plt.subplot2grid((1,1), (0,0))
    ax.plot(fpr, tpr, linewidth=2, label=label)
    ax.plot([0, 1], [0, 1], "k--")
    ax.axis([0, 1, 0, 1.01])
    plt.xlabel("False positive rate (fpr)")
    plt.ylabel("True positive rate (tpr)")
    
    return ax

"""#EXPLORING DATA """

# Code to read csv file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/content/drive')
link1 ='/content/drive/MyDrive/Colab Notebooks/DATA/Processed_DJI.csv'
link2 ='/content/drive/MyDrive/Colab Notebooks/DATA/Processed_NASDAQ.csv'
link3 ='/content/drive/MyDrive/Colab Notebooks/DATA/Processed_NYSE.csv'
link4 ='/content/drive/MyDrive/Colab Notebooks/DATA/Processed_RUSSELL.csv'
link5 ='/content/drive/MyDrive/Colab Notebooks/DATA/Processed_S&P.csv'
CLASS_NAME = "MOVEMENT"
DATASET_DJI = open(link1,)
DATASET_NASDAQ = open(link2,)
DATASET_NYSE = open(link3,)
DATASET_RUSSELL = open(link4,)
DATASET_SP = open(link5,)
market_orders, n_markets, aggregated_datasets = load_aggregated_datasets([DATASET_DJI, 
                                                                          DATASET_NASDAQ, 
                                                                          DATASET_NYSE,
                                                                          DATASET_RUSSELL, 
                                                                          DATASET_SP])

# Load datasets
## DJI
dji_df = aggregated_datasets["DJI"]
## NASDAQ
nasdaq_df = aggregated_datasets["NASDAQ"]
## NYSE
nyse_df = aggregated_datasets["NYA"]
## Russell
russell_df = aggregated_datasets["RUT"]
## SP
sp_df = aggregated_datasets["S&P"]

nyse_df.shape

#market_orders, n_markets, aggregated_datasets

"""CLEANING 

"""

## DJI
for col, n_null in zip(dji_df.columns, dji_df.isnull().sum()):
    print(f"{col}: {n_null}")
dji_df.shape[0] - dji_df.dropna().shape[0]

## NASDAQ
for col, n_null in zip(nasdaq_df.columns, nasdaq_df.isnull().sum()):
    print(f"{col}: {n_null}")

nasdaq_df.shape[0] - nasdaq_df.dropna().shape[0]

## RUSSELL
for col, n_null in zip(russell_df.columns, russell_df.isnull().sum()):
    print(f"{col}: {n_null}")
russell_df.shape[0] - russell_df.dropna().shape[0]


## NYSE
for col, n_null in zip(nyse_df.columns, nyse_df.isnull().sum()):
    print(f"{col}: {n_null}")

## S&P 500
for col, n_null in zip(sp_df.columns, sp_df.isnull().sum()):
    print(f"{col}: {n_null}")
sp_df.shape[0] - sp_df.dropna().shape[0]

# Fill missing values, do some scaling (run prev cell first)
list_df = []

for df in [dji_df, nasdaq_df, nyse_df, russell_df, sp_df]:
    columns = df.columns
    df.fillna(0, inplace=True) # fill na with 0
    y = df["MOVEMENT"].copy()
    X = df.drop(columns=["MOVEMENT"]).copy()
    scaler = StandardScaler()
    X = pd.DataFrame(scaler.fit_transform(X))
    X["MOVEMENT"] = np.array(y)
    X.columns = columns
    list_df.append(X)
    
### Clean dataframe (full features)
dji_df_full = list_df[0]
nasdaq_df_full = list_df[1]
nyse_df_full = list_df[2]
russell_df_full = list_df[3]
sp_df_full = list_df[4]

"""##In this section, we examine the attributes, including correlation with class variable, class distribution, key variables distributions."""

## DJI
corr_dji = dji_df_full.drop(columns=["MOVEMENT"]).corr()
print(corr_dji["Close"].sort_values(ascending=False))
print(corr_dji["EMA_10"].sort_values(ascending=False))
fig = plt.figure(figsize=(20, 30))
ax = fig.add_subplot(111)
mask=np.zeros_like(corr_dji)
mask[np.triu_indices_from(mask)] = True
sns.heatmap(corr_dji,
            xticklabels=corr_dji.columns,
            yticklabels=corr_dji.columns,
           ax=ax, mask=mask, square=True,
            cbar_kws={"shrink": 0.2},
           cmap="coolwarm")
plt.show()

## NASDAQ
corr_dji = nasdaq_df_full.drop(columns=["MOVEMENT"]).corr()
print(corr_dji["Close"].sort_values(ascending=False))
fig = plt.figure(figsize=(20, 30))
ax = fig.add_subplot(111)
mask=np.zeros_like(corr_dji)
mask[np.triu_indices_from(mask)] = True
sns.heatmap(corr_dji,
            xticklabels=corr_dji.columns,
            yticklabels=corr_dji.columns,
           ax=ax, mask=mask, square=True,
            cbar_kws={"shrink": 0.2},
           cmap="coolwarm")
plt.show()

## NYSE
corr_dji = nyse_df_full.drop(columns=["MOVEMENT"]).corr()
print(corr_dji["Close"].sort_values(ascending=False))
fig = plt.figure(figsize=(20, 30))
ax = fig.add_subplot(111)
mask=np.zeros_like(corr_dji)
mask[np.triu_indices_from(mask)] = True
sns.heatmap(corr_dji,
            xticklabels=corr_dji.columns,
            yticklabels=corr_dji.columns,
           ax=ax, mask=mask, square=True,
            cbar_kws={"shrink": 0.2},
           cmap="coolwarm")
plt.show()

## RUSSELL
corr_dji = russell_df_full.drop(columns=["MOVEMENT"]).corr()
print(corr_dji["Close"].sort_values(ascending=False))
fig = plt.figure(figsize=(20, 30))
ax = fig.add_subplot(111)
mask=np.zeros_like(corr_dji)
mask[np.triu_indices_from(mask)] = True
sns.heatmap(corr_dji,
            xticklabels=corr_dji.columns,
            yticklabels=corr_dji.columns,
           ax=ax, mask=mask, square=True,
            cbar_kws={"shrink": 0.2},
           cmap="coolwarm")
plt.show()

## S&P 500
corr_dji = sp_df_full.drop(columns=["MOVEMENT"]).corr()
print(corr_dji["Close"].sort_values(ascending=False))
fig = plt.figure(figsize=(20, 30))
ax = fig.add_subplot(111)
mask=np.zeros_like(corr_dji)
mask[np.triu_indices_from(mask)] = True
sns.heatmap(corr_dji,
            xticklabels=corr_dji.columns,
            yticklabels=corr_dji.columns,
           ax=ax, mask=mask, square=True,
            cbar_kws={"shrink": 0.2},
           cmap="coolwarm")
plt.show()

"""**NOTING CLASS DISTRIBUTIONS**"""

print('DJI VALUE COUNTS \n ',dji_df_full["MOVEMENT"].value_counts() ,' \n ')
print('nasdaq VALUE COUNTS \n ',nasdaq_df_full["MOVEMENT"].value_counts() ,' \n ')
print('nyse VALUE COUNTS \n ',nyse_df_full["MOVEMENT"].value_counts() ,' \n ')
print('russell VALUE COUNTS \n ',russell_df_full["MOVEMENT"].value_counts(),' \n ')
print('sp VALUE COUNTS \n ',sp_df_full["MOVEMENT"].value_counts(),' \n ' )

# Class distribution plot
dji_class = dji_df_full[["MOVEMENT"]]
dji_class["dataset_name"] = ["DJI" for i in range(dji_df_full.shape[0])]

nasdaq_class = nasdaq_df_full[["MOVEMENT"]]
nasdaq_class["dataset_name"] = ["NASDAQ" for i in range(dji_df_full.shape[0])]

nyse_class = nyse_df_full[["MOVEMENT"]]
nyse_class["dataset_name"] = ["NYSE" for i in range(dji_df_full.shape[0])]

russell_class = russell_df_full[["MOVEMENT"]]
russell_class["dataset_name"] = ["RUSSELL" for i in range(dji_df_full.shape[0])]

sp_class = sp_df_full[["MOVEMENT"]]
sp_class["dataset_name"] = ["S&P 500" for i in range(dji_df_full.shape[0])]

# Concatenate
merge_df = pd.concat([dji_class, nasdaq_class, nyse_class, russell_class, sp_class], ignore_index=True)

g = sns.catplot(x='dataset_name', hue='MOVEMENT', 
            kind='count', data=merge_df, ci=False, aspect=1.3,
               legend=False)
plt.xlabel("$Dataset$ $Name$")
plt.ylabel("$Count$")
plt.legend(labels=["Down (0)", "Up (1)"],
          bbox_to_anchor=[0.7,1.1], ncol=2, frameon=False)

plt.show()

## DJI NASDAQ, ETC
ax = dji_df_full.drop(columns=["MOVEMENT"]).hist(bins=50, alpha=0.5, label="Full", grid=False, figsize=(20, 25))
dji_df_full[dji_df_full["MOVEMENT"] == 0].hist(bins=50, ax=ax.ravel()[:83],grid=False, color="r", alpha=0.5, label="Down")
dji_df_full[dji_df_full["MOVEMENT"] == 1].hist(bins=50, ax=ax.ravel()[:83], color="gold",grid=False, alpha=0.5, label="Up")
plt.show()

"""##Feature selection wrt to labels


"""

fs = SelectKBest(score_func=f_classif)
X_selected = fs.fit_transform(dji_df_full.drop(columns=["MOVEMENT"]), dji_df_full["MOVEMENT"])

y = fs.scores_
x = dji_df_full.drop(columns=["MOVEMENT"]).columns

fig = plt.figure(figsize=(16, 7))
ax = fig.add_subplot(111)
sns.barplot(x=x, y=y, ax=ax)
plt.xticks(rotation=-90)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.ylabel("$Feature$ $Scores$")
plt.grid(axis="y")
plt.show()

## NASDAQ
fs = SelectKBest(score_func=f_classif)
X_selected = fs.fit_transform(nasdaq_df_full.drop(columns=["MOVEMENT"]), nasdaq_df_full["MOVEMENT"])
y = fs.scores_
x = nasdaq_df_full.drop(columns=["MOVEMENT"]).columns
fig = plt.figure(figsize=(16, 7))
ax = fig.add_subplot(111)
sns.barplot(x=x, y=y, ax=ax)
plt.xticks(rotation=-90)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.ylabel("$Feature$ $Scores$")
plt.grid(axis="y")
plt.show()

## RUSSELL
fs = SelectKBest(score_func=f_classif)
X_selected = fs.fit_transform(russell_df_full.drop(columns=["MOVEMENT"]), russell_df_full["MOVEMENT"])
y = fs.scores_
x = russell_df_full.drop(columns=["MOVEMENT"]).columns
fig = plt.figure(figsize=(16, 7))
ax = fig.add_subplot(111)
sns.barplot(x=x, y=y, ax=ax)
plt.xticks(rotation=-90)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.ylabel("$Feature$ $Scores$")
plt.grid(axis="y")
plt.show()

## S&P 500
fs = SelectKBest(score_func=f_classif)
X_selected = fs.fit_transform(sp_df_full.drop(columns=["MOVEMENT"]), sp_df_full["MOVEMENT"])
y = fs.scores_
x = sp_df_full.drop(columns=["MOVEMENT"]).columns
fig = plt.figure(figsize=(16, 7))
ax = fig.add_subplot(111)
sns.barplot(x=x, y=y, ax=ax)
plt.xticks(rotation=-90)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.ylabel("$Feature$ $Scores$")
plt.grid(axis="y")
plt.show()

"""##Time Series Data Exploration"""

## DJI
dji_df_time = dji_df.copy()
dji_df_time.fillna(0, inplace=True)
df1 = dji_df_time.loc["2011":"2012"]["Close"].copy().reset_index()["Close"]
df2 = dji_df_time.loc["2012":"2013"]["Close"].copy().reset_index()["Close"]
df3 = dji_df_time.loc["2013":"2014"]["Close"].copy().reset_index()["Close"]
df4 = dji_df_time.loc["2014":"2015"]["Close"].copy().reset_index()["Close"]
df5 = dji_df_time.loc["2015":"2016"]["Close"].copy().reset_index()["Close"]
df6 = dji_df_time.loc["2016":"2017"]["Close"].copy().reset_index()["Close"]
plt.figure(figsize=(14, 7))
ax = df1.plot(lw=2)
df2.plot(ax=ax, lw=2)
df3.plot(ax=ax, lw=2)
df4.plot(ax=ax, lw=2)
df5.plot(ax=ax, lw=2)
df6.plot(ax=ax, lw=2)
plt.xticks(np.arange(0, 252, 50), labels=["Jan", "Mar", "May", "Aug", "Oct", "Dec"])
ax.legend(["2011", "2012", "2013", "2014", "2015", "2016"],
         bbox_to_anchor=[0.8,1.1], ncol=6, frameon=False, prop={"size":12})
ax.tick_params(axis='both', which='major', labelsize=11)
ax.set_xlabel("$Date$", fontsize=14)
ax.set_ylabel("$Close$ $Price$", fontsize=14)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.grid(axis="y")
plt.show()

## NASDAQ
dji_df_time = nasdaq_df.copy()
dji_df_time.fillna(0, inplace=True)
df1 = dji_df_time.loc["2011":"2012"]["Close"].copy().reset_index()["Close"]
df2 = dji_df_time.loc["2012":"2013"]["Close"].copy().reset_index()["Close"]
df3 = dji_df_time.loc["2013":"2014"]["Close"].copy().reset_index()["Close"]
df4 = dji_df_time.loc["2014":"2015"]["Close"].copy().reset_index()["Close"]
df5 = dji_df_time.loc["2015":"2016"]["Close"].copy().reset_index()["Close"]
df6 = dji_df_time.loc["2016":"2017"]["Close"].copy().reset_index()["Close"]
plt.figure(figsize=(14, 7))
ax = df1.plot(lw=2)
df2.plot(ax=ax, lw=2)
df3.plot(ax=ax, lw=2)
df4.plot(ax=ax, lw=2)
df5.plot(ax=ax, lw=2)
df6.plot(ax=ax, lw=2)
plt.xticks(np.arange(0, 252, 50), labels=["Jan", "Mar", "May", "Aug", "Oct", "Dec"])
ax.legend(["2011", "2012", "2013", "2014", "2015", "2016"],
         bbox_to_anchor=[0.8,1.1], ncol=6, frameon=False, prop={"size":12})
ax.tick_params(axis='both', which='major', labelsize=11)
ax.set_xlabel("$Date$", fontsize=14)
ax.set_ylabel("$Close$ $Price$", fontsize=14)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.show()

## NYSE
dji_df_time = nyse_df.copy()
dji_df_time.fillna(0, inplace=True)
df1 = dji_df_time.loc["2011":"2012"]["Close"].copy().reset_index()["Close"]
df2 = dji_df_time.loc["2012":"2013"]["Close"].copy().reset_index()["Close"]
df3 = dji_df_time.loc["2013":"2014"]["Close"].copy().reset_index()["Close"]
df4 = dji_df_time.loc["2014":"2015"]["Close"].copy().reset_index()["Close"]
df5 = dji_df_time.loc["2015":"2016"]["Close"].copy().reset_index()["Close"]
df6 = dji_df_time.loc["2016":"2017"]["Close"].copy().reset_index()["Close"]
plt.figure(figsize=(14, 7))
ax = df1.plot(lw=2)
df2.plot(ax=ax, lw=2)
df3.plot(ax=ax, lw=2)
df4.plot(ax=ax, lw=2)
df5.plot(ax=ax, lw=2)
df6.plot(ax=ax, lw=2)
plt.xticks(np.arange(0, 252, 50), labels=["Jan", "Mar", "May", "Aug", "Oct", "Dec"])
ax.legend(["2011", "2012", "2013", "2014", "2015", "2016"],
         bbox_to_anchor=[0.8,1.1], ncol=6, frameon=False, prop={"size":12})
ax.tick_params(axis='both', which='major', labelsize=11)
ax.set_xlabel("$Date$", fontsize=14)
ax.set_ylabel("$Close$ $Price$", fontsize=14)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.show()

##russell
dji_df_time = russell_df.copy()
dji_df_time.fillna(0, inplace=True)
df1 = dji_df_time.loc["2011":"2012"]["Close"].copy().reset_index()["Close"]
df2 = dji_df_time.loc["2012":"2013"]["Close"].copy().reset_index()["Close"]
df3 = dji_df_time.loc["2013":"2014"]["Close"].copy().reset_index()["Close"]
df4 = dji_df_time.loc["2014":"2015"]["Close"].copy().reset_index()["Close"]
df5 = dji_df_time.loc["2015":"2016"]["Close"].copy().reset_index()["Close"]
df6 = dji_df_time.loc["2016":"2017"]["Close"].copy().reset_index()["Close"]
plt.figure(figsize=(14, 7))
ax = df1.plot(lw=2)
df2.plot(ax=ax, lw=2)
df3.plot(ax=ax, lw=2)
df4.plot(ax=ax, lw=2)
df5.plot(ax=ax, lw=2)
df6.plot(ax=ax, lw=2)
plt.xticks(np.arange(0, 252, 50), labels=["Jan", "Mar", "May", "Aug", "Oct", "Dec"])
ax.legend(["2011", "2012", "2013", "2014", "2015", "2016"],
         bbox_to_anchor=[0.8,1.1], ncol=6, frameon=False, prop={"size":12})
ax.tick_params(axis='both', which='major', labelsize=11)
ax.set_xlabel("$Date$", fontsize=14)
ax.set_ylabel("$Close$ $Price$", fontsize=14)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.show()

## s&p 500
dji_df_time = sp_df.copy()
dji_df_time.fillna(0, inplace=True)
df1 = dji_df_time.loc["2011":"2012"]["Close"].copy().reset_index()["Close"]
df2 = dji_df_time.loc["2012":"2013"]["Close"].copy().reset_index()["Close"]
df3 = dji_df_time.loc["2013":"2014"]["Close"].copy().reset_index()["Close"]
df4 = dji_df_time.loc["2014":"2015"]["Close"].copy().reset_index()["Close"]
df5 = dji_df_time.loc["2015":"2016"]["Close"].copy().reset_index()["Close"]
df6 = dji_df_time.loc["2016":"2017"]["Close"].copy().reset_index()["Close"]
plt.figure(figsize=(14, 7))
ax = df1.plot(lw=2)
df2.plot(ax=ax, lw=2)
df3.plot(ax=ax, lw=2)
df4.plot(ax=ax, lw=2)
df5.plot(ax=ax, lw=2)
df6.plot(ax=ax, lw=2)
plt.xticks(np.arange(0, 252, 50), labels=["Jan", "Mar", "May", "Aug", "Oct", "Dec"])
ax.legend(["2011", "2012", "2013", "2014", "2015", "2016"],
         bbox_to_anchor=[0.8,1.1], ncol=6, frameon=False, prop={"size":12})
ax.tick_params(axis='both', which='major', labelsize=11)
ax.set_xlabel("$Date$", fontsize=14)
ax.set_ylabel("$Close$ $Price$", fontsize=14)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.show()

#Auto CORRELATION PLOT

## DJI, NASDAQ, ETC
fig = plt.figure(figsize=(14, 7))
dji_df_time = dji_df.copy()
dji_df_time.fillna(0, inplace=True)
ax = pd.plotting.autocorrelation_plot(dji_df_time["2011":"2016"]["Close"], lw=2)
ax.set_xlabel("$Lag$", fontsize=14)
ax.set_ylabel("$Autocorrelation$", fontsize=14)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.show()

"""#Experimenting"""

import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras import layers, models, backend as K, callbacks

"""##LOADING AND PREPROCESSING AGAIN:: MIGHT HAVE USED THE SAME VARIABLES AGAIN"""

# Load datasets
## DJI
dji_df = aggregated_datasets["DJI"]

## NASDAQ
nasdaq_df = aggregated_datasets["NASDAQ"]

## NYSE
nyse_df = aggregated_datasets["NYA"]

## Russell
russell_df = aggregated_datasets["RUT"]

## SP
sp_df = aggregated_datasets["S&P"]

# Build sequential dataset
sequence_length = 60

### Sequential dataset (full features)
dji_X_seq, dji_y_seq = generate_sequential_data(dji_df_full, sequence_length)
nasdaq_X_seq, nasdaq_y_seq = generate_sequential_data(nasdaq_df_full, sequence_length)
nyse_X_seq, nyse_y_seq = generate_sequential_data(nyse_df_full, sequence_length)
russell_X_seq, russell_y_seq = generate_sequential_data(russell_df_full, sequence_length)
sp_X_seq, sp_y_seq = generate_sequential_data(sp_df_full, sequence_length)



########################################################################
#DATA PRE PROC
##########################################################################
# Sequential flatten (full features)
dji_X_seq_flatten = sequential_reshape(dji_X_seq, (len(dji_X_seq), -1))
nasdaq_X_seq_flatten = sequential_reshape(nasdaq_X_seq, (len(nasdaq_X_seq), -1))
nyse_X_seq_flatten = sequential_reshape(nyse_X_seq, (len(nyse_X_seq), -1))
russell_X_seq_flatten = sequential_reshape(russell_X_seq, (len(russell_X_seq), -1))
sp_X_seq_flatten = sequential_reshape(sp_X_seq, (len(sp_X_seq), -1))
# Split into training, validation, and test set.
# It should be noted that we should not shuffle to prevent
# cheating the value of an index from data from other index
# on the same date.
dji_X_train_full, dji_X_test_full, dji_y_train_full, dji_y_test_full = train_test_split(dji_X_seq_flatten,
                                                                                        dji_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
dji_X_train_full, dji_X_valid_full, dji_y_train_full, dji_y_valid_full = train_test_split(dji_X_train_full,
                                                                                        dji_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nasdaq_X_train_full, nasdaq_X_test_full, nasdaq_y_train_full, nasdaq_y_test_full = train_test_split(nasdaq_X_seq_flatten,
                                                                                        nasdaq_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nasdaq_X_train_full, nasdaq_X_valid_full, nasdaq_y_train_full, nasdaq_y_valid_full = train_test_split(nasdaq_X_train_full,
                                                                                        nasdaq_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nyse_X_train_full, nyse_X_test_full, nyse_y_train_full, nyse_y_test_full = train_test_split(nyse_X_seq_flatten,
                                                                                        nyse_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1, 
                                                                                        shuffle=False)
nyse_X_train_full, nyse_X_valid_full, nyse_y_train_full, nyse_y_valid_full = train_test_split(nyse_X_train_full,
                                                                                        nyse_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, 
                                                                                        shuffle=False)
russell_X_train_full, russell_X_test_full, russell_y_train_full, russell_y_test_full = train_test_split(russell_X_seq_flatten,
                                                                                        russell_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1, 
                                                                                        shuffle=False)
russell_X_train_full, russell_X_valid_full, russell_y_train_full, russell_y_valid_full = train_test_split(russell_X_train_full,
                                                                                        russell_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, 
                                                                                        shuffle=False)
sp_X_train_full, sp_X_test_full, sp_y_train_full, sp_y_test_full = train_test_split(sp_X_seq_flatten,
                                                                                        sp_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
sp_X_train_full, sp_X_valid_full, sp_y_train_full, sp_y_valid_full = train_test_split(sp_X_train_full,
                                                                                        sp_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, 
                                                                                        shuffle=False)

# Merge indices into one (training)
X_train_full = np.concatenate(
    (dji_X_train_full,
     nasdaq_X_train_full,
     nyse_X_train_full,
     russell_X_train_full,
     sp_X_train_full)
)

y_train_full = np.concatenate(
    (np.array(dji_y_train_full),
     np.array(nasdaq_y_train_full),
     np.array(nyse_y_train_full),
     np.array(russell_y_train_full),
     np.array(sp_y_train_full))
)

# Validation
X_valid_full = np.concatenate(
    (dji_X_valid_full,
     nasdaq_X_valid_full,
     nyse_X_valid_full,
     russell_X_valid_full,
     sp_X_valid_full)
)

y_valid_full = np.concatenate(
    (np.array(dji_y_valid_full),
     np.array(nasdaq_y_valid_full),
     np.array(nyse_y_valid_full),
     np.array(russell_y_valid_full),
     np.array(sp_y_valid_full))
)

X_train_full.shape

"""#Perform and visualise random search result.

param_grid = {
    "C": np.random.uniform(1, 100, 20),
    "solver": ["lbfgs", "saga", "sag"],
    "fit_intercept": [True, False]
}

dict_result = {"C": [], "solver": [], "fit_intercept": [],
              "accuracy": [], "f1": []} # store grid search result


param_grid["C"] = list(param_grid["C"])
param_grid["C"].append(1) # Add default value
# Search over params on the param_grid
for c in tqdm(param_grid["C"]):
    for solver in param_grid["solver"]:
        for intercept in param_grid["fit_intercept"]:
            print(f"C: {c}, solver: {solver}, fit_intercept:{intercept}")
            dict_result["C"].append(c)
            dict_result["solver"].append(solver)
            dict_result["fit_intercept"].append(intercept)
            
            # Build logistic regression
            lr = LogisticRegression(C=c, fit_intercept=intercept, solver=solver)
            lr.fit(X_train_full, y_train_full)
            
            pred = lr.predict(X_valid_full)
            print(f"Accuracy: {accuracy_score(pred, y_valid_full)}")            
            print(f"Macro F1: {f1_score(pred, y_valid_full, average='macro')}")
            print("*"*50, "\n")
            dict_result["accuracy"].append(accuracy_score(pred, y_valid_full))
            dict_result["f1"].append(f1_score(pred, y_valid_full, average='macro'))

##Feedforward network
"""

# Sequential flatten (full features)
dji_X_seq_flatten = sequential_reshape(dji_X_seq, (len(dji_X_seq), -1))
nasdaq_X_seq_flatten = sequential_reshape(nasdaq_X_seq, (len(nasdaq_X_seq), -1))
nyse_X_seq_flatten = sequential_reshape(nyse_X_seq, (len(nyse_X_seq), -1))
russell_X_seq_flatten = sequential_reshape(russell_X_seq, (len(russell_X_seq), -1))
sp_X_seq_flatten = sequential_reshape(sp_X_seq, (len(sp_X_seq), -1))

#Split into training/validation/test (80/10/10)
## Full features


dji_X_train_full, dji_X_test_full, dji_y_train_full, dji_y_test_full = train_test_split(dji_X_seq_flatten,
                                                                                        dji_y_seq,
                                                                                        stratify=dji_y_seq,
                                                                                        test_size=0.1,
                                                                                        shuffle=True)
dji_X_train_full, dji_X_valid_full, dji_y_train_full, dji_y_valid_full = train_test_split(dji_X_train_full,
                                                                                        dji_y_train_full,
                                                                                        stratify=dji_y_train_full,
                                                                                        test_size=0.1,
                                                                                        shuffle=True)
nasdaq_X_train_full, nasdaq_X_test_full, nasdaq_y_train_full, nasdaq_y_test_full = train_test_split(nasdaq_X_seq_flatten,
                                                                                        nasdaq_y_seq,
                                                                                        stratify=nasdaq_y_seq,
                                                                                        test_size=0.1,
                                                                                        shuffle=True)
nasdaq_X_train_full, nasdaq_X_valid_full, nasdaq_y_train_full, nasdaq_y_valid_full = train_test_split(nasdaq_X_train_full,
                                                                                        nasdaq_y_train_full,
                                                                                        stratify=nasdaq_y_train_full,
                                                                                        test_size=0.1,
                                                                                        shuffle=True)
nyse_X_train_full, nyse_X_test_full, nyse_y_train_full, nyse_y_test_full = train_test_split(nyse_X_seq_flatten,
                                                                                        nyse_y_seq,
                                                                                        stratify=nyse_y_seq,
                                                                                        test_size=0.1, shuffle=True)
nyse_X_train_full, nyse_X_valid_full, nyse_y_train_full, nyse_y_valid_full = train_test_split(nyse_X_train_full,
                                                                                        nyse_y_train_full,
                                                                                        stratify=nyse_y_train_full,
                                                                                        test_size=0.1, shuffle=True)

russell_X_train_full, russell_X_test_full, russell_y_train_full, russell_y_test_full = train_test_split(russell_X_seq_flatten,
                                                                                        russell_y_seq,
                                                                                        stratify=russell_y_seq,
                                                                                        test_size=0.1, shuffle=True)
russell_X_train_full, russell_X_valid_full, russell_y_train_full, russell_y_valid_full = train_test_split(russell_X_train_full,
                                                                                        russell_y_train_full,
                                                                                        stratify=russell_y_train_full,
                                                                                        test_size=0.1, shuffle=True)
sp_X_train_full, sp_X_test_full, sp_y_train_full, sp_y_test_full = train_test_split(sp_X_seq_flatten,
                                                                                        sp_y_seq,
                                                                                        stratify=sp_y_seq,
                                                                                        test_size=0.1, shuffle=True)
sp_X_train_full, sp_X_valid_full, sp_y_train_full, sp_y_valid_full = train_test_split(sp_X_train_full,
                                                                                        sp_y_train_full,
                                                                                        stratify=sp_y_train_full,
                                                                                        test_size=0.1, shuffle=True)

"""training and EVAL

"""

## DJI
model = keras.Sequential(
    [
        layers.Dense(64, activation="relu"),
        layers.Dense(32, activation="relu"),
        layers.Dense(1, activation="sigmoid"),
    ]
)

optimizer = keras.optimizers.Adam(lr=1e-2)
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=50, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False
)

model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit(dji_X_train_full, np.array(dji_y_train_full), 
          epochs=100, batch_size=32,
          validation_data=(dji_X_valid_full, np.array(dji_y_valid_full)), callbacks=[early_stopping])

result_dji_full = model.predict(dji_X_test_full)
result_dji_full = (result_dji_full > 0.5).astype(int)
print(f"DJI Accuracy: {accuracy_score(result_dji_full, dji_y_test_full)}")
print(f"DJI F1: {f1_score(result_dji_full, dji_y_test_full, average='macro')}")

history = model.fit(dji_X_train_full, np.array(dji_y_train_full), 
          epochs=100, batch_size=32,
          validation_data=(dji_X_valid_full, np.array(dji_y_valid_full)), callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()

plot_accuracy(history, miny=0)

## NASDAQ
model = keras.Sequential(
    [
        layers.Dense(64, activation="relu"),
        layers.Dense(32, activation="relu"),
        layers.Dense(1, activation="sigmoid"),
    ]
)

optimizer = keras.optimizers.Adam()
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=100, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False
)

model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit(nasdaq_X_train_full, np.array(nasdaq_y_train_full), 
          epochs=100, batch_size=128,
          validation_data=(nasdaq_X_valid_full, np.array(nasdaq_y_valid_full)), callbacks=[early_stopping])

## NASDAQ
result_nasdaq_full = model.predict(nasdaq_X_test_full)
result_nasdaq_full = (result_nasdaq_full > 0.5).astype(int)
print(f"NASDAQ Accuracy: {accuracy_score(result_nasdaq_full, nasdaq_y_test_full)}")
print(f"NASDAQ F1: {f1_score(result_nasdaq_full, nasdaq_y_test_full, average='macro')}")

history = model.fit(nasdaq_X_train_full, np.array(nasdaq_y_train_full), 
          epochs=100, batch_size=128,
          validation_data=(nasdaq_X_valid_full, np.array(nasdaq_y_valid_full)), callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()

plot_accuracy(history, miny=0)

## NYSE
model = keras.Sequential(
    [
        layers.Dense(64, activation="relu"),
        layers.Dense(32, activation="relu"),
        layers.Dense(1, activation="sigmoid"),
    ]
)

optimizer = keras.optimizers.Adam()
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=100, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False
)

model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit(nyse_X_train_full, np.array(nyse_y_train_full), 
          epochs=100, batch_size=32,
          validation_data=(nyse_X_valid_full, np.array(nyse_y_valid_full)), callbacks=[early_stopping])

## NYSE
result_nyse_full = model.predict(nyse_X_test_full)
result_nyse_full = (result_nyse_full > 0.5).astype(int)
print(f"NYSE Accuracy: {accuracy_score(result_nyse_full, nyse_y_test_full)}")
print(f"NYSE F1: {f1_score(result_nyse_full, nyse_y_test_full, average='macro')}")

history=model.fit(nyse_X_train_full, np.array(nyse_y_train_full), 
          epochs=100, batch_size=32,
          validation_data=(nyse_X_valid_full, np.array(nyse_y_valid_full)), callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()

plot_accuracy(history, miny=0)

## Russell
model = keras.Sequential(
    [
        layers.Dense(64, activation="relu"),
        layers.Dense(32, activation="relu"),
        layers.Dense(1, activation="sigmoid"),
    ]
)

optimizer = keras.optimizers.Adam()
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=50, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False
)

model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit(russell_X_train_full, np.array(russell_y_train_full), 
          epochs=100, batch_size=128,
          validation_data=(russell_X_valid_full, np.array(russell_y_valid_full)), callbacks=[early_stopping])

## russell
result_russell_full = model.predict(nyse_X_test_full)
result_russell_full = (result_russell_full > 0.5).astype(int)
print(f"russell Accuracy: {accuracy_score(result_russell_full, russell_y_test_full)}")
print(f"russell F1: {f1_score(result_russell_full, russell_y_test_full, average='macro')}")

history=model.fit(russell_X_train_full, np.array(russell_y_train_full), 
          epochs=100, batch_size=128,
          validation_data=(russell_X_valid_full, np.array(russell_y_valid_full)), callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)

## S&P 500
model = keras.Sequential(
    [
        layers.Dense(64, activation="relu"),
        layers.Dense(32, activation="relu"),
        layers.Dense(1, activation="sigmoid"),
    ]
)

optimizer = keras.optimizers.Adam()
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=100, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False
)

model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit(sp_X_train_full, np.array(sp_y_train_full), 
          epochs=100, batch_size=128,
          validation_data=(sp_X_valid_full, np.array(sp_y_valid_full)), callbacks=[early_stopping])

## russell
result_sp_full = model.predict(nyse_X_test_full)
result_sp_full = (result_sp_full > 0.5).astype(int)
print(f"NYSE Accuracy: {accuracy_score(result_sp_full, sp_y_test_full)}")
print(f"NYSE F1: {f1_score(result_sp_full, sp_y_test_full, average='macro')}")

history=model.fit(sp_X_train_full, np.array(sp_y_train_full), 
          epochs=100, batch_size=128,
          validation_data=(sp_X_valid_full, np.array(sp_y_valid_full)), callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)

accurracy_FF=[accuracy_score(result_dji_full, dji_y_test_full),accuracy_score(result_nasdaq_full, nasdaq_y_test_full),accuracy_score(result_nyse_full, nyse_y_test_full),accuracy_score(result_russell_full, russell_y_test_full),accuracy_score(result_sp_full, sp_y_test_full)]
f1_score_FF=[f1_score(result_dji_full, dji_y_test_full),f1_score(result_nasdaq_full, nasdaq_y_test_full),f1_score(result_nyse_full, nyse_y_test_full),f1_score(result_russell_full, russell_y_test_full),f1_score(result_sp_full, sp_y_test_full)]

new_zip=(f1_score_FF,accurracy_FF)
bars=['DJI','NASDAQ','nyse','RUSSEL','S&P']

raw=pd.DataFrame({
    'bars':bars,
    'f1_score_FF':f1_score_FF,
     'accurracy_FF':accurracy_FF,
     })
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn
raw.plot(x="bars", y=["f1_score_FF", "accurracy_FF"], kind="bar")
plt.xticks(rotation=45)

"""##**3d CNN**"""

### Clean dataframe (full features)
dji_df_full = dji_df_full
nasdaq_df_full = nasdaq_df_full
nyse_df_full = nyse_df_full
russell_df_full = russell_df_full
sp_df_full = sp_df_full

## Combine
X_full = dji_df_full.drop(columns=["MOVEMENT"]).copy()
y_full = list(dji_df_full["MOVEMENT"].copy())

for df in [nasdaq_df_full, nyse_df_full, russell_df_full, sp_df_full]:
    X_full = np.concatenate((X_full, df.drop(columns=["MOVEMENT"]).copy()))
    y_full.extend(list(df["MOVEMENT"].copy()))

## Reshape
X_full = X_full.reshape((5, -1, 82))
y_full = np.array(y_full)

## Sequencing
X_seq, y_seq = generate_sequential_data_3d(X_full, y_full, 60)

X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(
    X_seq, y_seq, stratify=None, test_size=0.1, shuffle=False
)
X_seq_train, X_seq_valid, y_seq_train, y_seq_valid = train_test_split(
    X_seq_train, y_seq_train, stratify=None, test_size=0.1, shuffle=False
)

model_full = cnnpred_3d(5, 60, 82, [8, 8, 8])
epochs = 100
batch_size=128

model_full.compile(optimizer="Adam", loss="mae", 
                   metrics=["acc", f1])
model_full.fit(X_seq_train, y_seq_train, epochs=100,
              batch_size=batch_size,
              validation_data=(X_seq_valid, y_seq_valid))

result_full = model_full.predict(X_seq_test)
result_full = (result_full > 0.5).astype(int)
print(f"FULL Accuracy: {accuracy_score(result_full, y_seq_test)}")
print(f"FULL F1: {f1_score(result_full, y_seq_test, average='macro')}")

from tensorflow.keras.optimizers import RMSprop

history = model_full.fit(X_seq_train, y_seq_train, epochs=100,
              batch_size=batch_size,
              validation_data=(X_seq_valid, y_seq_valid))

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.plot(epochs, acc)
  plt.plot(epochs, test_acc)
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()

plot_accuracy(history, miny=0)

def cnnpred_3d_g(n_markets, sequence_length, n_feature, n_filters):
    """
    Build model using architecture that is specified on the paper
    (Hoseinzade and Haratizadeh).
    """
    
    model = keras.Sequential([
        # layer 1
        layers.Conv2D(n_filters[0], (1, 1), activation='relu', 
                      input_shape=(n_markets,sequence_length,n_feature), data_format='channels_last'),
        
        # layer 2
        layers.Conv2D(n_filters[1], (n_markets, 3), activation="relu"),
        layers.MaxPool2D(pool_size=(1, 2)),
        
        # layer 3
        layers.Conv2D(n_filters[2], (1, 3), activation="relu"),
        layers.MaxPool2D(pool_size=(1, 2)),

          # layer 4
        layers.Conv2D(n_filters[3], (1, 3), activation="relu"),
        layers.MaxPool2D(pool_size=(1, 2)),

          # layer 5
        layers.Conv2D(n_filters[2], (1, 3), activation="relu"),
        layers.MaxPool2D(pool_size=(1, 2)),
        
        # FFNN
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(200, activation='relu'),
        layers.Dense(1, activation='softmax')
    ])
    
    return model

model_full = cnnpred_3d_g(5, 60, 82, [8, 8, 8])
# epochs = 100
# batch_size=128

model_full.compile(optimizer=RMSprop(lr=0.001), loss="categorical_crossentropy", metrics=["acc", f1])
history= model_full.fit(X_seq_train, y_seq_train, epochs=10, batch_size=20, validation_data=(X_seq_valid, y_seq_valid))

plot_accuracy(history, miny=0)

"""##**LSTM **"""

### DJI
# Split
X = np.array(dji_df_full.drop(columns=["MOVEMENT"]).copy())
y = np.array(dji_df_full["MOVEMENT"].copy())

# Split train test, should not shuffle as the data is time series
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, shuffle=False
)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train, y_train, test_size=0.1, shuffle=False
)

# Get input ready for lstm
win_length = 60
batch_size = 32
num_features = 82
train_generator = TimeseriesGenerator(np.array(X_train), np.array(y_train),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

test_generator = TimeseriesGenerator(np.array(X_test), np.array(y_test),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

valid_generator = TimeseriesGenerator(np.array(X_valid), np.array(y_valid),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

model = lstm(win_length, num_features)

optimizer = keras.optimizers.Adam()

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                 patience=20,
                                                 mode="min")
model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])
result_dji_full = model.predict(test_generator)
result_dji_full = (result_dji_full > 0.5).astype(int)
print(f"DJI Accuracy: {accuracy_score(result_dji_full, y_test[win_length:])}")
print(f"DJI F1: {f1_score(result_dji_full, y_test[win_length:], average='macro')}")

history = model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)

### NASDAQ
# Split
X = np.array(nasdaq_df_full.drop(columns=["MOVEMENT"]).copy())
y = np.array(nasdaq_df_full["MOVEMENT"].copy())

# Split train test, should not shuffle as the data is time series
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, shuffle=False
)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train, y_train, test_size=0.1, shuffle=False
)
# Get input ready for lstm
win_length = 60
batch_size = 32
num_features = 82
train_generator = TimeseriesGenerator(np.array(X_train), np.array(y_train),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

test_generator = TimeseriesGenerator(np.array(X_test), np.array(y_test),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

valid_generator = TimeseriesGenerator(np.array(X_valid), np.array(y_valid),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

model = lstm(win_length, num_features)

optimizer = keras.optimizers.Adam()

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                 patience=20,
                                                 mode="min")
model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

result_nasdaq_full = model.predict(test_generator)
result_nasdaq_full = (result_nasdaq_full > 0.5).astype(int)
print(f"NASDAQ Accuracy: {accuracy_score(result_nasdaq_full, y_test[win_length:])}")
print(f"NASDAQ F1: {f1_score(result_nasdaq_full, y_test[win_length:], average='macro')}")

history=model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)

### NYSE
# Split
X = np.array(nyse_df_full.drop(columns=["MOVEMENT"]).copy())
y = np.array(nyse_df_full["MOVEMENT"].copy())

# Split train test, should not shuffle as the data is time series
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, shuffle=False
)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train, y_train, test_size=0.1, shuffle=False
)
# Get input ready for lstm
win_length = 60
batch_size = 32
num_features = 82
train_generator = TimeseriesGenerator(np.array(X_train), np.array(y_train),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

test_generator = TimeseriesGenerator(np.array(X_test), np.array(y_test),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

valid_generator = TimeseriesGenerator(np.array(X_valid), np.array(y_valid),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

model = lstm(win_length, num_features)

optimizer = keras.optimizers.Adam()

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                 patience=20,
                                                 mode="min")
model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

result_nyse_full = model.predict(test_generator)
result_nyse_full = (result_nyse_full > 0.5).astype(int)
print(f"NYSE Accuracy: {accuracy_score(result_nyse_full, y_test[win_length:])}")
print(f"Nyse F1: {f1_score(result_nyse_full, y_test[win_length:], average='macro')}")

history=model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)

### russell
# Split
X = np.array(russell_df_full.drop(columns=["MOVEMENT"]).copy())
y = np.array(russell_df_full["MOVEMENT"].copy())

# Split train test, should not shuffle as the data is time series
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, shuffle=False
)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train, y_train, test_size=0.1, shuffle=False
)
# Get input ready for lstm
win_length = 60
batch_size = 32
num_features = 82
train_generator = TimeseriesGenerator(np.array(X_train), np.array(y_train),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

test_generator = TimeseriesGenerator(np.array(X_test), np.array(y_test),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

valid_generator = TimeseriesGenerator(np.array(X_valid), np.array(y_valid),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

model = lstm(win_length, num_features)

optimizer = keras.optimizers.Adam()

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                 patience=20,
                                                 mode="min")
model.compile(optimizer=optimizer, loss="binary_crossentropy",
                   metrics=["acc", f1])
model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

result_russell_full = model.predict(test_generator)
result_russell_full = (result_russell_full > 0.5).astype(int)
print(f"RUSSELL Accuracy: {accuracy_score(result_russell_full, y_test[win_length:])}")
print(f"RUSSELL F1: {f1_score(result_russell_full, y_test[win_length:], average='macro')}")

history=model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)

### sp
# Split
X = np.array(sp_df_full.drop(columns=["MOVEMENT"]).copy())
y = np.array(sp_df_full["MOVEMENT"].copy())

# Split train test, should not shuffle as the data is time series
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, shuffle=False
)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train, y_train, test_size=0.1, shuffle=False
)

# Get input ready for lstm
win_length = 60
batch_size = 32
num_features = 82
train_generator = TimeseriesGenerator(np.array(X_train), np.array(y_train),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

test_generator = TimeseriesGenerator(np.array(X_test), np.array(y_test),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

valid_generator = TimeseriesGenerator(np.array(X_valid), np.array(y_valid),
                                     length=win_length,
                                     sampling_rate=1,
                                     batch_size=batch_size)

model = lstm(win_length, num_features)

optimizer = keras.optimizers.Adam()

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                 patience=20,
                                                 mode="min")
model.compile(optimizer=optimizer, loss="binary_crossentropy", 
                   metrics=["acc", f1])
model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

result_sp_full = model.predict(test_generator)
result_sp_full = (result_dji_full > 0.5).astype(int)
print(f"SP Accuracy: {accuracy_score(result_sp_full, y_test[win_length:])}")
print(f"SP F1: {f1_score(result_sp_full, y_test[win_length:], average='macro')}")

history=model.fit_generator(train_generator,
                   epochs=100,
                   validation_data=valid_generator,
                   shuffle=False, callbacks=[early_stopping])

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)

accurracy_LSTM=[accuracy_score(result_dji_full,y_test[win_length:]),accuracy_score(result_nasdaq_full,  y_test[win_length:]),accuracy_score(result_nyse_full,  y_test[win_length:]),accuracy_score(result_russell_full,  y_test[win_length:]),accuracy_score(result_sp_full,  y_test[win_length:])]
f1_score_LSTM=[f1_score(result_dji_full, y_test[win_length:], average='macro'),f1_score(result_nasdaq_full, y_test[win_length:], average='macro'),f1_score(result_nyse_full, y_test[win_length:], average='macro'),f1_score(result_russell_full, y_test[win_length:], average='macro'),f1_score(result_sp_full, y_test[win_length:], average='macro')]

new_zip=(f1_score_LSTM,accurracy_LSTM)
bars=['DJI','NASDAQ','nyse','RUSSEL','S&P']

raw=pd.DataFrame({
    'bars':bars,
    'f1_score_LSTM':f1_score_LSTM,
     'accurracy_LSTM':accurracy_LSTM,
     })
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn
raw.plot(x="bars", y=["f1_score_LSTM", "accurracy_LSTM"], kind="bar")
plt.xticks(rotation=45)

"""##**2d_CNN**"""

#LOAD DATA 
# Sequential flatten (full features)
dji_X_seq_flatten = sequential_reshape(dji_X_seq, (len(dji_X_seq), sequence_length, -1, 1))
nasdaq_X_seq_flatten = sequential_reshape(nasdaq_X_seq, (len(dji_X_seq), sequence_length, -1, 1))
nyse_X_seq_flatten = sequential_reshape(nyse_X_seq, (len(dji_X_seq), sequence_length, -1, 1))
russell_X_seq_flatten = sequential_reshape(russell_X_seq, (len(dji_X_seq), sequence_length, -1, 1))
sp_X_seq_flatten = sequential_reshape(sp_X_seq, (len(dji_X_seq), sequence_length, -1, 1))

#80/10/10
## Full features
dji_X_train_full, dji_X_test_full, dji_y_train_full, dji_y_test_full = train_test_split(dji_X_seq_flatten,
                                                                                        dji_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
dji_X_train_full, dji_X_valid_full, dji_y_train_full, dji_y_valid_full = train_test_split(dji_X_train_full,
                                                                                        dji_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nasdaq_X_train_full, nasdaq_X_test_full, nasdaq_y_train_full, nasdaq_y_test_full = train_test_split(nasdaq_X_seq_flatten,
                                                                                        nasdaq_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nasdaq_X_train_full, nasdaq_X_valid_full, nasdaq_y_train_full, nasdaq_y_valid_full = train_test_split(nasdaq_X_train_full,
                                                                                        nasdaq_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nyse_X_train_full, nyse_X_test_full, nyse_y_train_full, nyse_y_test_full = train_test_split(nyse_X_seq_flatten,
                                                                                        nyse_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
nyse_X_train_full, nyse_X_valid_full, nyse_y_train_full, nyse_y_valid_full = train_test_split(nyse_X_train_full,
                                                                                        nyse_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)

russell_X_train_full, russell_X_test_full, russell_y_train_full, russell_y_test_full = train_test_split(russell_X_seq_flatten,
                                                                                        russell_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
russell_X_train_full, russell_X_valid_full, russell_y_train_full, russell_y_valid_full = train_test_split(russell_X_train_full,
                                                                                        russell_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
sp_X_train_full, sp_X_test_full, sp_y_train_full, sp_y_test_full = train_test_split(sp_X_seq_flatten,
                                                                                        sp_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
sp_X_train_full, sp_X_valid_full, sp_y_train_full, sp_y_valid_full = train_test_split(sp_X_train_full,
                                                                                        sp_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)

print(dji_X_train_full.shape)
print(dji_X_valid_full.shape)
print(dji_X_test_full.shape)

#combine all the data sets
# full
# Train
X_train_full = np.concatenate(
    (dji_X_train_full,
     nasdaq_X_train_full,
     nyse_X_train_full,
     russell_X_train_full,
     sp_X_train_full)
)
y_train_full = np.concatenate(
    (np.array(dji_y_train_full),
     np.array(nasdaq_y_train_full),
     np.array(nyse_y_train_full),
     np.array(russell_y_train_full),
     np.array(sp_y_train_full)
    )
)

# Valid
X_valid_full = np.concatenate(
    (dji_X_valid_full,
     nasdaq_X_valid_full,
     nyse_X_valid_full,
     russell_X_valid_full,
     sp_X_valid_full)
)
y_valid_full = np.concatenate(
    (np.array(dji_y_valid_full),
     np.array(nasdaq_y_valid_full),
     np.array(nyse_y_valid_full),
     np.array(russell_y_valid_full),
     np.array(sp_y_valid_full)
    )
)

model_full = cnnpred_2d(60, 82, [8, 8, 8])
epochs = 100
batch_size=128

## Training
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=100, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False
)

model_full.compile(optimizer="Adam", loss="binary_crossentropy", 
                   metrics=["acc", f1])
model_full.fit(X_train_full, y_train_full, epochs=epochs,
              batch_size=batch_size, callbacks=[early_stopping],
              validation_data=(X_valid_full, y_valid_full))

## DJI
result_dji_full = model_full.predict(dji_X_test_full)
result_dji_full = (result_dji_full > 0.5).astype(int)
print(f"DJI Accuracy: {accuracy_score(result_dji_full, dji_y_test_full)}")
print(f"DJI F1: {f1_score(result_dji_full, dji_y_test_full, average='macro')}")

## NASDAQ
result_nasdaq_full = model_full.predict(nasdaq_X_test_full)
result_nasdaq_full = (result_nasdaq_full > 0.5).astype(int)
print(f"NASDAQ Accuracy: {accuracy_score(result_nasdaq_full, nasdaq_y_test_full)}")
print(f"NASDAQ F1: {f1_score(result_nasdaq_full, nasdaq_y_test_full, average='macro')}")

## nyse
result_nyse_full = model_full.predict(nyse_X_test_full)
result_nyse_full = (result_nyse_full > 0.5).astype(int)
print(f"nyse Accuracy: {accuracy_score(result_nyse_full, nyse_y_test_full)}")
print(f"nyse F1: {f1_score(result_nyse_full, nyse_y_test_full, average='macro')}")

## russell
result_russell_full = model_full.predict(russell_X_test_full)
result_russell_full = (result_russell_full > 0.5).astype(int)
print(f"russell Accuracy: {accuracy_score(result_russell_full, russell_y_test_full)}")
print(f"russell F1: {f1_score(result_russell_full, russell_y_test_full, average='macro')}")

## sp
result_sp_full = model_full.predict(sp_X_test_full)
result_sp_full = (result_sp_full > 0.5).astype(int)
print(f"S&P 500 Accuracy: {accuracy_score(result_sp_full, sp_y_test_full)}")
print(f"S&P 500 F1: {f1_score(result_sp_full, sp_y_test_full, average='macro')}")

accurracy_2D_CNN=[accuracy_score(result_dji_full, dji_y_test_full),accuracy_score(result_nasdaq_full, nasdaq_y_test_full),accuracy_score(result_nyse_full, nyse_y_test_full),accuracy_score(result_russell_full, russell_y_test_full),accuracy_score(result_sp_full, sp_y_test_full)]
f1_score_2D_CNN=[f1_score(result_dji_full, dji_y_test_full, average='macro'),f1_score(result_nasdaq_full, nasdaq_y_test_full, average='macro'),f1_score(result_nyse_full, nyse_y_test_full, average='macro'),f1_score(result_russell_full, russell_y_test_full, average='macro'),f1_score(result_sp_full, sp_y_test_full, average='macro')]

new_zip=(f1_score_2D_CNN,accurracy_2D_CNN)
bars=['DJI','NASDAQ','nyse','RUSSEL','S&P']

raw=pd.DataFrame({
    'bars':bars,
    'f1_score_2D_CNN':f1_score_2D_CNN,
     'accurracy_2D_CNN':accurracy_2D_CNN,
     })
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn
raw.plot(x="bars", y=["f1_score_2D_CNN", "accurracy_2D_CNN"], kind="bar")
plt.xticks(rotation=45)

history=model_full.fit(X_train_full, y_train_full, epochs=epochs,
              batch_size=batch_size, callbacks=[early_stopping],
              validation_data=(X_valid_full, y_valid_full))

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)

"""#TCN

https://www.kaggle.com/christofhenkel/temporal-convolutional-network
"""

!pip install wget
!pip install keras-tcn
import wget
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense
from tqdm.notebook import tqdm
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import ModelCheckpoint, EarlyStopping
from tcn import TCN

#LOAD DATA 
# Sequential flatten (full features)
dji_X_seq_flatten = sequential_reshape(dji_X_seq, (len(dji_X_seq), sequence_length, -1, 1))
nasdaq_X_seq_flatten = sequential_reshape(nasdaq_X_seq, (len(dji_X_seq), sequence_length, -1, 1))
nyse_X_seq_flatten = sequential_reshape(nyse_X_seq, (len(dji_X_seq), sequence_length, -1, 1))
russell_X_seq_flatten = sequential_reshape(russell_X_seq, (len(dji_X_seq), sequence_length, -1, 1))
sp_X_seq_flatten = sequential_reshape(sp_X_seq, (len(dji_X_seq), sequence_length, -1, 1))

#80/10/10
## Full features
dji_X_train_full, dji_X_test_full, dji_y_train_full, dji_y_test_full = train_test_split(dji_X_seq_flatten,
                                                                                        dji_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
dji_X_train_full, dji_X_valid_full, dji_y_train_full, dji_y_valid_full = train_test_split(dji_X_train_full,
                                                                                        dji_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nasdaq_X_train_full, nasdaq_X_test_full, nasdaq_y_train_full, nasdaq_y_test_full = train_test_split(nasdaq_X_seq_flatten,
                                                                                        nasdaq_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nasdaq_X_train_full, nasdaq_X_valid_full, nasdaq_y_train_full, nasdaq_y_valid_full = train_test_split(nasdaq_X_train_full,
                                                                                        nasdaq_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1,
                                                                                        shuffle=False)
nyse_X_train_full, nyse_X_test_full, nyse_y_train_full, nyse_y_test_full = train_test_split(nyse_X_seq_flatten,
                                                                                        nyse_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
nyse_X_train_full, nyse_X_valid_full, nyse_y_train_full, nyse_y_valid_full = train_test_split(nyse_X_train_full,
                                                                                        nyse_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)

russell_X_train_full, russell_X_test_full, russell_y_train_full, russell_y_test_full = train_test_split(russell_X_seq_flatten,
                                                                                        russell_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
russell_X_train_full, russell_X_valid_full, russell_y_train_full, russell_y_valid_full = train_test_split(russell_X_train_full,
                                                                                        russell_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
sp_X_train_full, sp_X_test_full, sp_y_train_full, sp_y_test_full = train_test_split(sp_X_seq_flatten,
                                                                                        sp_y_seq,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
sp_X_train_full, sp_X_valid_full, sp_y_train_full, sp_y_valid_full = train_test_split(sp_X_train_full,
                                                                                        sp_y_train_full,
                                                                                        stratify=None,
                                                                                        test_size=0.1, shuffle=False)
print(dji_X_train_full.shape)
print(dji_X_valid_full.shape)
print(dji_X_test_full.shape)

#combine all the data sets
# full
# Train
X_train_full = np.concatenate(
    (dji_X_train_full,
     nasdaq_X_train_full,
     nyse_X_train_full,
     russell_X_train_full,
     sp_X_train_full)
)
y_train_full = np.concatenate(
    (np.array(dji_y_train_full),
     np.array(nasdaq_y_train_full),
     np.array(nyse_y_train_full),
     np.array(russell_y_train_full),
     np.array(sp_y_train_full)
    )
)

# Valid
X_valid_full = np.concatenate(
    (dji_X_valid_full,
     nasdaq_X_valid_full,
     nyse_X_valid_full,
     russell_X_valid_full,
     sp_X_valid_full)
)
y_valid_full = np.concatenate(
    (np.array(dji_y_valid_full),
     np.array(nasdaq_y_valid_full),
     np.array(nyse_y_valid_full),
     np.array(russell_y_valid_full),
     np.array(sp_y_valid_full)
    )
)

lookback_window = 12

dji_X_train_full.shape[-2]

"""epochs = 200
batch_size=128

## Training
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=100, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False
)

model_full.compile(optimizer="Adam", loss="binary_crossentropy", 
                   metrics=["acc", f1])
model_full.fit(X_train_full, y_train_full, epochs=epochs,
              batch_size=batch_size, callbacks=[early_stopping],
              validation_data=(X_valid_full, y_valid_full))
"""

X_train_full.shape[-2]

#i= Input(shape=(80, 62, 1))
i = Input(shape=(60, 82))
m = TCN()(i)
m = Dense(1, activation = 'sigmoid')(m)
#early_stopping = EarlyStopping(patience = 50, restore_best_weights=True, min_delta = 0.000)
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=100, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False
)
reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=10, min_lr=0.00001)
model_TCN = Model(inputs=[i], outputs=[m])
model_TCN.summary()
#model_full.compile(optimizer="Adam", loss="binary_crossentropy", metrics=["acc", f1])
model_TCN.compile(loss = "binary_crossentropy", optimizer = tf.keras.optimizers.Adam(lr = 1e-3),metrics=["acc", f1])
#model_TCN.reset_states()
#model_TCN.fit(X_train_full, y_train_full,validation_data = (X_valid_full, y_valid_full),shuffle = False, callbacks = [early_stopping, reduceLR],batch_size = 8, epochs = 200)

model_TCN.fit(X_train_full, y_train_full, epochs=100, batch_size=8, callbacks=[early_stopping,reduceLR],shuffle = False,validation_data=(X_valid_full, y_valid_full))


#model_TCN.compile(optimizer="Adam", loss="binary_crossentropy", metrics=["acc", f1])
#model_TCN.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size, callbacks=[early_stopping],validation_data=(X_valid_full, y_valid_full))

## DJI
result_dji_full = model_TCN.predict(dji_X_test_full)
result_dji_full = (result_dji_full > 0.5).astype(int)
print(f"DJI Accuracy: {accuracy_score(result_dji_full, dji_y_test_full)}")
print(f"DJI F1: {f1_score(result_dji_full, dji_y_test_full, average='macro')}")

## NASDAQ
result_nasdaq_full = model_TCN.predict(nasdaq_X_test_full)
result_nasdaq_full = (result_nasdaq_full > 0.5).astype(int)
print(f"NASDAQ Accuracy: {accuracy_score(result_nasdaq_full, nasdaq_y_test_full)}")
print(f"NASDAQ F1: {f1_score(result_nasdaq_full, nasdaq_y_test_full, average='macro')}")

## nyse
result_nyse_full = model_TCN.predict(nyse_X_test_full)
result_nyse_full = (result_nyse_full > 0.5).astype(int)
print(f"nyse Accuracy: {accuracy_score(result_nyse_full, nyse_y_test_full)}")
print(f"nyse F1: {f1_score(result_nyse_full, nyse_y_test_full, average='macro')}")

## russell
result_russell_full = model_TCN.predict(russell_X_test_full)
result_russell_full = (result_russell_full > 0.5).astype(int)
print(f"russell Accuracy: {accuracy_score(result_russell_full, russell_y_test_full)}")
print(f"russell F1: {f1_score(result_russell_full, russell_y_test_full, average='macro')}")

## sp
result_sp_full = model_TCN.predict(sp_X_test_full)
result_sp_full = (result_sp_full > 0.5).astype(int)
print(f"S&P 500 Accuracy: {accuracy_score(result_sp_full, sp_y_test_full)}")
print(f"S&P 500 F1: {f1_score(result_sp_full, sp_y_test_full, average='macro')}")

accurracy_TCN=[accuracy_score(result_dji_full, dji_y_test_full),accuracy_score(result_nasdaq_full, nasdaq_y_test_full),accuracy_score(result_nyse_full, nyse_y_test_full),accuracy_score(result_russell_full, russell_y_test_full),accuracy_score(result_sp_full, sp_y_test_full)]
f1_score_TCN=[f1_score(result_dji_full, dji_y_test_full, average='macro'),f1_score(result_nasdaq_full, nasdaq_y_test_full, average='macro'),f1_score(result_nyse_full, nyse_y_test_full, average='macro'),f1_score(result_russell_full, russell_y_test_full, average='macro'),f1_score(result_sp_full, sp_y_test_full, average='macro')]

new_zip=(f1_score_TCN,accurracy_TCN)
bars=['DJI','NASDAQ','nyse','RUSSEL','S&P']

raw=pd.DataFrame({
    'bars':bars,
    'f1_score_TCN':f1_score_TCN,
     'accurracy_TCN':accurracy_TCN,
     })
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn
raw.plot(x="bars", y=["f1_score_TCN", "accurracy_TCN"], kind="bar")
plt.xticks(rotation=45)

history=model_TCN.fit(X_train_full, y_train_full, epochs=100, batch_size=8, callbacks=[early_stopping,reduceLR],shuffle = False,validation_data=(X_valid_full, y_valid_full))

def plot_accuracy(history, miny=None):
  acc = history.history['acc']
  test_acc = history.history['val_acc']
  epochs = range(len(acc))
  plt.legend()
  plt.plot(epochs, acc, label="Train Data")
  plt.plot(epochs, test_acc, label="Val Data")
  if miny:
    plt.ylim(miny, 1.0)
    plt.title('accuracy') 
    plt.figure()
    plt.show()


plot_accuracy(history, miny=0)
